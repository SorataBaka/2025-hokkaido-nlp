@inproceedings{zoltan2020ner,
  author    = {Zoltán Papp},
  title     = {Task-Adaptive Pretraining for Named Entity Recognition},
  booktitle = {Proceedings of the Conference on Computational Linguistics},
  year      = {2020},
  pages     = {123--134}
}

@article{houman2021sentiment,
  author    = {Houman Mohebbi},
  title     = {Data Requirements for Fine-Tuning BERT on Sentiment Analysis},
  journal   = {Journal of Natural Language Engineering},
  year      = {2021},
  volume    = {27},
  number    = {3},
  pages     = {451--468}
}
@article{mujahid2024data,
  title={Data oversampling and imbalanced datasets: an investigation of performance for machine learning and feature engineering},
  author={Muhammad Mujahid and Erol Kına and Furqan Rustam and Monica Gracia Villar and Eduardo Silva Alvarado and Isabel De La Torre Diez and Imran Ashraf},
  journal={Journal of Big Data},
  volume={11},
  number={87},
  pages={1--22},
  year={2024},
  doi={10.1186/s40537-024-00943-4},
  url={https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00943-4}
}
@article{THOLKE2023120253,
title = {Class imbalance should not throw you off balance: Choosing the right classifiers and performance metrics for brain decoding with imbalanced data},
journal = {NeuroImage},
volume = {277},
pages = {120253},
year = {2023},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2023.120253},
url = {https://www.sciencedirect.com/science/article/pii/S1053811923004044},
author = {Philipp Thölke and Yorguin-Jose Mantilla-Ramos and Hamza Abdelhedi and Charlotte Maschke and Arthur Dehgan and Yann Harel and Anirudha Kemtur and Loubna {Mekki Berrada} and Myriam Sahraoui and Tammy Young and Antoine {Bellemare Pépin} and Clara {El Khantour} and Mathieu Landry and Annalisa Pascarella and Vanessa Hadid and Etienne Combrisson and Jordan O’Byrne and Karim Jerbi},
keywords = {Class imbalance, Machine learning, Classification, Performance metrics, Electroencephalography, Magnetoencephalography, Brain decoding, Balanced accuracy},
abstract = {Machine learning (ML) is increasingly used in cognitive, computational and clinical neuroscience. The reliable and efficient application of ML requires a sound understanding of its subtleties and limitations. Training ML models on datasets with imbalanced classes is a particularly common problem, and it can have severe consequences if not adequately addressed. With the neuroscience ML user in mind, this paper provides a didactic assessment of the class imbalance problem and illustrates its impact through systematic manipulation of data imbalance ratios in (i) simulated data and (ii) brain data recorded with electroencephalography (EEG), magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI). Our results illustrate how the widely-used Accuracy (Acc) metric, which measures the overall proportion of successful predictions, yields misleadingly high performances, as class imbalance increases. Because Acc weights the per-class ratios of correct predictions proportionally to class size, it largely disregards the performance on the minority class. A binary classification model that learns to systematically vote for the majority class will yield an artificially high decoding accuracy that directly reflects the imbalance between the two classes, rather than any genuine generalizable ability to discriminate between them. We show that other evaluation metrics such as the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC), and the less common Balanced Accuracy (BAcc) metric - defined as the arithmetic mean between sensitivity and specificity, provide more reliable performance evaluations for imbalanced data. Our findings also highlight the robustness of Random Forest (RF), and the benefits of using stratified cross-validation and hyperprameter optimization to tackle data imbalance. Critically, for neuroscience ML applications that seek to minimize overall classification error, we recommend the routine use of BAcc, which in the specific case of balanced data is equivalent to using standard Acc, and readily extends to multi-class settings. Importantly, we present a list of recommendations for dealing with imbalanced data, as well as open-source code to allow the neuroscience community to replicate and extend our observations and explore alternative approaches to coping with imbalanced data.}
}

@inproceedings{nagarajan2023token,
  author    = {Amrit Nagarajan and Anand Raghunathan},
  title     = {TokenDrop + BucketSampler: Towards Efficient Padding-Free Fine-Tuning of Language Models},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  year      = {2023},
  url       = {https://aclanthology.org/2023.findings-emnlp.782/},
  note      = {Accessed: 2025-08-18}
}
@article{liu2024bucket,
  title={Bucket Pre-training is All You Need},
  author={Hongtao Liu and Qiyao Peng and Qing Yang and Kai Liu and Hongyan Xu},
  journal={arXiv preprint arXiv:2407.07495},
  year={2024},
  url={https://arxiv.org/abs/2407.07495},
  note={Accessed: 2025-08-18}
}
@article{petukhova2025text,
  title   = {Text Clustering with Large Language Model Embeddings},
  author  = {Petukhova, Alina and Matos-Carvalho, João P. and Fachada, Nuno},
  journal = {International Journal of Cognitive Computing in Engineering},
  volume  = {6},
  pages   = {100--108},
  year    = {2025},
  doi     = {10.1016/j.ijcce.2024.11.004},
  url     = {https://doi.org/10.1016/j.ijcce.2024.11.004}
}

@article{dao2024transformers,
  title={Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert and Rush, Alexander},
  journal={arXiv preprint arXiv:2406.08128},
  year={2024},
  url={https://arxiv.org/abs/2406.08128}
}
@article{MAJDIK2024,
title = {Sample Size Considerations for Fine-Tuning Large Language Models for Named Entity Recognition Tasks: Methodological Study},
journal = {JMIR AI},
volume = {3},
year = {2024},
issn = {2817-1705},
doi = {https://doi.org/10.2196/52095},
url = {https://www.sciencedirect.com/science/article/pii/S2817170524000279},
author = {Zoltan P Majdik and S Scott Graham and Jade C {Shiva Edward} and Sabrina N Rodriguez and Martha S Karnes and Jared T Jensen and Joshua B Barbour and Justin F Rousseau},
keywords = {named-entity recognition, large language models, fine-tuning, transfer learning, expert annotation, annotation, sample size, sample, language model, machine learning, natural language processing, disclosure, disclosures, statement, statements, conflict of interest},
abstract = {Background
Large language models (LLMs) have the potential to support promising new applications in health informatics. However, practical data on sample size considerations for fine-tuning LLMs to perform specific tasks in biomedical and health policy contexts are lacking.
Objective
This study aims to evaluate sample size and sample selection techniques for fine-tuning LLMs to support improved named entity recognition (NER) for a custom data set of conflicts of interest disclosure statements.
Methods
A random sample of 200 disclosure statements was prepared for annotation. All “PERSON” and “ORG” entities were identified by each of the 2 raters, and once appropriate agreement was established, the annotators independently annotated an additional 290 disclosure statements. From the 490 annotated documents, 2500 stratified random samples in different size ranges were drawn. The 2500 training set subsamples were used to fine-tune a selection of language models across 2 model architectures (Bidirectional Encoder Representations from Transformers [BERT] and Generative Pre-trained Transformer [GPT]) for improved NER, and multiple regression was used to assess the relationship between sample size (sentences), entity density (entities per sentence [EPS]), and trained model performance (F1-score). Additionally, single-predictor threshold regression models were used to evaluate the possibility of diminishing marginal returns from increased sample size or entity density.
Results
Fine-tuned models ranged in topline NER performance from F1-score=0.79 to F1-score=0.96 across architectures. Two-predictor multiple linear regression models were statistically significant with multiple R2 ranging from 0.6057 to 0.7896 (all P<.001). EPS and the number of sentences were significant predictors of F1-scores in all cases ( P<.001), except for the GPT-2_large model, where EPS was not a significant predictor (P=.184). Model thresholds indicate points of diminishing marginal return from increased training data set sample size measured by the number of sentences, with point estimates ranging from 439 sentences for RoBERTa_large to 527 sentences for GPT-2_large. Likewise, the threshold regression models indicate a diminishing marginal return for EPS with point estimates between 1.36 and 1.38.
Conclusions
Relatively modest sample sizes can be used to fine-tune LLMs for NER tasks applied to biomedical text, and training data entity density should representatively approximate entity density in production data. Training data quality and a model architecture’s intended use (text generation vs text processing or classification) may be as, or more, important as training data volume and model parameter size.}
}

@misc{merrillees2021stratifiedsamplingextrememultilabel,
      title={Stratified Sampling for Extreme Multi-Label Data}, 
      author={Maximillian Merrillees and Lan Du},
      year={2021},
      eprint={2103.03494},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2103.03494}, 
}

@article{QuTuBao2024,
  title   = {Is Semantic Chunking Worth the Computational Cost?},
  author  = {Qu, Renyi and Tu, Ruixuan and Bao, Forrest},
  journal = {arXiv preprint arXiv:2410.13070},
  year    = {2024},
  url     = {https://arxiv.org/abs/2410.13070}
}
@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{zhang2024textbfonlyifrevealingdecisiveeffectinstruction,
      title={$\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization}, 
      author={Dylan Zhang and Justin Wang and Francois Charton},
      year={2024},
      eprint={2410.04717},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.04717}, 
}

@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}

@article{peterfreund2021multidimensional,
  title={Multidimensional scaling of noisy high-dimensional data},
  author={Peterfreund, E.},
  journal={Computational Statistics \& Data Analysis},
  volume={154},
  pages={106352},
  year={2021},
  publisher={Elsevier},
  doi={10.1016/j.csda.2020.106352}
}

@article{mcinnes2018umap,
  title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}

@article{IKOTUN2023178,
title = {K-means clustering algorithms: A comprehensive review, variants analysis, and advances in the era of big data},
journal = {Information Sciences},
volume = {622},
pages = {178-210},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.139},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522014633},
author = {Abiodun M. Ikotun and Absalom E. Ezugwu and Laith Abualigah and Belal Abuhaija and Jia Heming},
keywords = {K-means, K-means variants, Clustering algorithm, Modified k-means, Improved k-means, Perspectives on big data clustering, Big data clustering},
abstract = {Advances in recent techniques for scientific data collection in the era of big data allow for the systematic accumulation of large quantities of data at various data-capturing sites. Similarly, exponential growth in the development of different data analysis approaches has been reported in the literature, amongst which the K-means algorithm remains the most popular and straightforward clustering algorithm. The broad applicability of the algorithm in many clustering application areas can be attributed to its implementation simplicity and low computational complexity. However, the K-means algorithm has many challenges that negatively affect its clustering performance. In the algorithm’s initialization process, users must specify the number of clusters in a given dataset apriori while the initial cluster centers are randomly selected. Furthermore, the algorithm's performance is susceptible to the selection of this initial cluster and for large datasets, determining the optimal number of clusters to start with becomes complex and is a very challenging task. Moreover, the random selection of the initial cluster centers sometimes results in minimal local convergence due to its greedy nature. A further limitation is that certain data object features are used in determining their similarity by using the Euclidean distance metric as a similarity measure, but this limits the algorithm’s robustness in detecting other cluster shapes and poses a great challenge in detecting overlapping clusters. Many research efforts have been conducted and reported in literature with regard to improving the K-means algorithm’s performance and robustness. The current work presents an overview and taxonomy of the K-means clustering algorithm and its variants. The history of the K-means, current trends, open issues and challenges, and recommended future research perspectives are also discussed.}
}
@inproceedings{ethayarajh-2019-contextual,
    title = "How Contextual are Contextualized Word Representations? {C}omparing the Geometry of {BERT}, {ELM}o, and {GPT}-2 Embeddings",
    author = "Ethayarajh, Kawin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1006/",
    doi = "10.18653/v1/D19-1006",
    pages = "55--65",
    abstract = "Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5{\%} of the variance in a word{'}s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations."
}
@InProceedings{10.1007/978-3-642-37456-2_14,
author="Campello, Ricardo J. G. B.
and Moulavi, Davoud
and Sander, Joerg",
editor="Pei, Jian
and Tseng, Vincent S.
and Cao, Longbing
and Motoda, Hiroshi
and Xu, Guandong",
title="Density-Based Clustering Based on Hierarchical Density Estimates",
booktitle="Advances in Knowledge Discovery and Data Mining",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="160--172",
abstract="We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a ``flat'' partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data.",
isbn="978-3-642-37456-2"
}


@article{ROUSSEEUW198753,
title = {Silhouettes: A graphical aid to the interpretation and validation of cluster analysis},
journal = {Journal of Computational and Applied Mathematics},
volume = {20},
pages = {53-65},
year = {1987},
issn = {0377-0427},
doi = {https://doi.org/10.1016/0377-0427(87)90125-7},
url = {https://www.sciencedirect.com/science/article/pii/0377042787901257},
author = {Peter J. Rousseeuw},
keywords = {Graphical display, cluster analysis, clustering validity, classification},
abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate’ number of clusters.}
}
@article{davies1979cluster,
  title={A cluster separation measure},
  author={Davies, David L. and Bouldin, Donald W.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={PAMI-1},
  number={2},
  pages={224--227},
  year={1979},
  publisher={IEEE}
}
@article{calinski1974dendrite,
  title={A dendrite method for cluster analysis},
  author={Calinski, T. and Harabasz, J.},
  journal={Communications in Statistics},
  volume={3},
  number={1},
  pages={1--27},
  year={1974},
  publisher={Taylor \& Francis}
}
@article{muennighoff2022mteb, author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils}, title = {MTEB: Massive Text Embedding Benchmark}, publisher = {arXiv}, journal={arXiv preprint arXiv:2210.07316}, year = {2022} url = {https://arxiv.org/abs/2210.07316}, doi = {10.48550/ARXIV.2210.07316}, }

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4171--4186},
  year={2019}
}
@article{wongoutong2024feature,
  author = {Wongoutong, C.},
  title = {The impact of neglecting feature scaling in k-means clustering},
  journal = {PLoS One},
  year = {2024},
  month = dec,
  day = {6},
  volume = {19},
  number = {12},
  pages = {e0310839},
  doi = {10.1371/journal.pone.0310839},
  pmid = {39642177},
  pmcid = {PMC11623793}
}
@inproceedings{shah-etal-2020-predictive,
    title = "Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview",
    author = "Shah, Deven Santosh  and
      Schwartz, H. Andrew  and
      Hovy, Dirk",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.468/",
    doi = "10.18653/v1/2020.acl-main.468",
    pages = "5248--5264",
    abstract = "An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks."
}