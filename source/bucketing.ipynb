{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c06dd16",
   "metadata": {},
   "source": [
    "\n",
    "There are 2 stages to this experiment. First is to bucket the dataset and second is to train based on entropy score. \n",
    "Bucketing the dataset involves the following:\n",
    "1. using ``google-bert/bert-base-uncased``, we will tokenize the text.\n",
    "2. On default, ``bert-base-uncased`` creates embeddings that are in 768 dimensions.\n",
    "3. Embeddings are then reduced in size to around 50.\n",
    "4. Extract n amount from each bucket to create the first training stage dataset.\n",
    "\n",
    "Training based on entropy score involves the following:\n",
    "1. Remove the previously trained data from the dataset pool.\n",
    "2. Predict on the rest of the dataset pool using the previously trained data in stage 1.\n",
    "3. Convert prediction scores to entropy\n",
    "4. Sort by highest entropy, seperate into buckets again.\n",
    "5. Extract m amount from each bucket to create the second training stage dataset.\n",
    "  \n",
    "Entropy based training can be done multiple times until sufficient.\n",
    "\n",
    "For each stage, we can compare it to a model trained on a randomly selected amount of data from the dataset pool. \n",
    "\n",
    "Since we are trying to prove that it is possible to create a high quality model without a huge amount of data, for each comparison, we can select the same amount of data used to train the bucketing model up to that point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf69dc",
   "metadata": {},
   "source": [
    "# Load and preprocess dataset\n",
    "We are trying to prove that bucketing and entropy based training can reduce the amount of data needed to reduce a well balanced model. This is a case of one-shot model training. To reduce external factors such as bad data quality, etc; we will be utilizing StanfordNLP's SST2 dataset which  is a standard NLP benchmark for sentiment classification. The dataset will be loaded from huggingface via ``stanfordnlp/sst2``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba9b15a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"stanfordnlp/sst2\", cache_dir=\"caches/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a3b88",
   "metadata": {},
   "source": [
    "Let's split into 3 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fac0cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds[\"train\"]\n",
    "validation_ds = ds[\"validation\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e866f",
   "metadata": {},
   "source": [
    "Perform very light cleaning on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2cb5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(row):\n",
    "  text = str(row[\"sentence\"])\n",
    "  text = text.lower()\n",
    "  text = text.strip()\n",
    "  return {\"sentence\": text}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349c0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(clean_text)\n",
    "validation_ds = validation_ds.map(clean_text)\n",
    "test_ds = test_ds.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e42af39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'sentence', 'label', 'mean_pooled_embeddings'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2568c3",
   "metadata": {},
   "source": [
    "# Generate embeddings and reduce size\n",
    "As mentioned before, embeddings are generated using ```google-bert/bert-base-uncased``` model. As bert embeddings are usually massive (768 dimensions), we will reduce it to around 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba8a93",
   "metadata": {},
   "source": [
    "Define the tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8893eb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"caches/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"caches/\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "456495f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_mean_pooled_embeddings(batch):\n",
    "  inputs = tokenizer(\n",
    "    batch[\"sentence\"],\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\"\n",
    "  )\n",
    "  inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "  with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "  last_hidden = outputs.last_hidden_state\n",
    "  mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "  mean_pool = (last_hidden * mask).sum(dim = 1) / mask.sum(dim = 1)\n",
    "  return {\n",
    "    \"mean_pooled_embeddings\": mean_pool.cpu().numpy()\n",
    "  }\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33b996a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 67349/67349 [54:30<00:00, 20.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = train_ds.map(get_mean_pooled_embeddings, batch_size=16, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ab1d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"bert_mean_pooled_embeddings.npy\", train_ds[\"mean_pooled_embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df1cb47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pooled_bert_embeddings = np.load(\"bert_mean_pooled_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "54cfd3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "pca = UMAP(n_components=50, random_state=42)\n",
    "reduced_embeddings = pca.fit_transform(train_ds[\"mean_pooled_embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c921d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "normalized_embeddings = normalize(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e264c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=2 | Silhouette: 0.3800 | Davies-Bouldin: 1.0437 | Calinski-Harabasz: 52930.6392\n",
      "k=3 | Silhouette: 0.3063 | Davies-Bouldin: 1.3173 | Calinski-Harabasz: 40233.3084\n",
      "k=4 | Silhouette: 0.2598 | Davies-Bouldin: 1.3586 | Calinski-Harabasz: 34546.7694\n",
      "k=5 | Silhouette: 0.2464 | Davies-Bouldin: 1.3461 | Calinski-Harabasz: 29691.6326\n",
      "k=6 | Silhouette: 0.2266 | Davies-Bouldin: 1.4336 | Calinski-Harabasz: 28159.9143\n",
      "k=7 | Silhouette: 0.2251 | Davies-Bouldin: 1.3965 | Calinski-Harabasz: 25222.0860\n",
      "k=8 | Silhouette: 0.2120 | Davies-Bouldin: 1.4910 | Calinski-Harabasz: 23692.4232\n",
      "k=9 | Silhouette: 0.2199 | Davies-Bouldin: 1.4527 | Calinski-Harabasz: 22377.6393\n",
      "k=10 | Silhouette: 0.2127 | Davies-Bouldin: 1.4501 | Calinski-Harabasz: 20965.0288\n",
      "k=11 | Silhouette: 0.2205 | Davies-Bouldin: 1.4122 | Calinski-Harabasz: 20405.1481\n",
      "k=12 | Silhouette: 0.2188 | Davies-Bouldin: 1.4270 | Calinski-Harabasz: 19272.5394\n",
      "k=13 | Silhouette: 0.2237 | Davies-Bouldin: 1.3693 | Calinski-Harabasz: 19044.0544\n",
      "k=14 | Silhouette: 0.2242 | Davies-Bouldin: 1.3631 | Calinski-Harabasz: 18210.1235\n",
      "k=15 | Silhouette: 0.2229 | Davies-Bouldin: 1.3428 | Calinski-Harabasz: 17476.2590\n",
      "k=16 | Silhouette: 0.2241 | Davies-Bouldin: 1.3398 | Calinski-Harabasz: 17178.6525\n",
      "k=17 | Silhouette: 0.2245 | Davies-Bouldin: 1.3305 | Calinski-Harabasz: 16725.2784\n",
      "k=18 | Silhouette: 0.2196 | Davies-Bouldin: 1.3768 | Calinski-Harabasz: 16138.8222\n",
      "k=19 | Silhouette: 0.2245 | Davies-Bouldin: 1.3432 | Calinski-Harabasz: 15816.9971\n",
      "k=20 | Silhouette: 0.2224 | Davies-Bouldin: 1.3540 | Calinski-Harabasz: 15324.1960\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "for k in range(2, 21):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(normalized_embeddings)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    sil_score = silhouette_score(normalized_embeddings, labels)\n",
    "    db_score = davies_bouldin_score(normalized_embeddings, labels)\n",
    "    ch_score = calinski_harabasz_score(normalized_embeddings, labels)\n",
    "    \n",
    "    print(f\"k={k} | Silhouette: {sil_score:.4f} | Davies-Bouldin: {db_score:.4f} | Calinski-Harabasz: {ch_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
