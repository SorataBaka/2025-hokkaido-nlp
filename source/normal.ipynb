{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bdb0e32",
   "metadata": {},
   "source": [
    "# Load datasets, clean, and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7529873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"mteb/amazon_polarity\", cache_dir=\"caches/\")\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ce23cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(row):\n",
    "    text = row[\"text\"]\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    # Remove non-printable characters\n",
    "    text = re.sub(r\"[\\x00-\\x1F\\x7F]\", \" \", text)\n",
    "    # Replace multiple spaces/newlines with single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Optionally lowercase\n",
    "    text = text.strip()  # Don't lowercase if case matters\n",
    "    return {\n",
    "        \"text\": text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8558c8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DS length: 36000\n",
      "Test DS length: 4000\n"
     ]
    }
   ],
   "source": [
    "train_valid = train_ds.train_test_split(test_size=0.01, seed=42)\n",
    "test_valid = test_ds.train_test_split(test_size=0.01, seed=42)\n",
    "\n",
    "train_ds_reduced = train_valid[\"test\"]\n",
    "test_ds_reduced = test_valid[\"test\"]\n",
    "\n",
    "train_ds_reduced = train_ds_reduced.map(clean_text)\n",
    "test_ds_reduced = test_ds_reduced.map(clean_text)\n",
    "\n",
    "print(f\"Train DS length: {len(train_ds_reduced)}\")\n",
    "print(f\"Test DS length: {len(test_ds_reduced)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a10e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_reduced = train_ds_reduced.shuffle(seed=42).select(range(4281))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba06d701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text', 'label_text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb9814d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"caches/\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"caches/\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "730033ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(batch):\n",
    "  tokens =  tokenizer(\n",
    "    batch[\"text\"],\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256\n",
    "  )\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "676be9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4281/4281 [00:00<00:00, 6861.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = train_ds_reduced.map(get_tokens, batch_size=16, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c997cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "final_dataset_train = final_dataset_split[\"train\"]\n",
    "final_dataset_test = final_dataset_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea338579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "dl = DataLoader(tokenized_dataset, batch_size=4, collate_fn=default_data_collator)\n",
    "\n",
    "for batch in dl:\n",
    "    print(batch['input_ids'].shape)  # ❌ Likely to crash or be malformed\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37cb0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gc import callbacks\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./results\",\n",
    "  num_train_epochs=10,\n",
    "  per_device_train_batch_size=8,\n",
    "  save_strategy=\"epoch\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=50,\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"eval_accuracy\",\n",
    "  greater_is_better=True,\n",
    "  eval_strategy=\"epoch\",\n",
    "  warmup_ratio=0.1,\n",
    "  weight_decay=0.01,\n",
    "  learning_rate=3e-5,\n",
    "  lr_scheduler_type=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c511022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, default_data_collator\n",
    "from torch.optim import AdamW\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import EarlyStoppingCallback, Trainer\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\", pos_label=1, zero_division=0\n",
    "    )\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        \"eval_accuracy\": acc,\n",
    "        \"eval_precision\": precision,\n",
    "        \"eval_recall\": recall,\n",
    "        \"eval_f1\": f1,\n",
    "    }\n",
    "  \n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args = training_args,\n",
    "  train_dataset=final_dataset_train,\n",
    "  eval_dataset=final_dataset_test,\n",
    "  compute_metrics=compute_metrics,\n",
    "  data_collator=default_data_collator,\n",
    "  optimizers=(AdamW(model.parameters(), lr=5e-5), None),\n",
    "  callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.01)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2ace6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1446' max='4820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1446/4820 16:44 < 39:07, 1.44 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.298500</td>\n",
       "      <td>0.294690</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.942105</td>\n",
       "      <td>0.844340</td>\n",
       "      <td>0.890547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.224000</td>\n",
       "      <td>0.362331</td>\n",
       "      <td>0.906760</td>\n",
       "      <td>0.943299</td>\n",
       "      <td>0.863208</td>\n",
       "      <td>0.901478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.469772</td>\n",
       "      <td>0.906760</td>\n",
       "      <td>0.887387</td>\n",
       "      <td>0.929245</td>\n",
       "      <td>0.907834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1446, training_loss=0.23768880027623263, metrics={'train_runtime': 1006.0304, 'train_samples_per_second': 38.289, 'train_steps_per_second': 4.791, 'total_flos': 1520255677870080.0, 'train_loss': 0.23768880027623263, 'epoch': 3.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7698fb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianharjuno/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Confusion Matrix ===\n",
      "[[1893   69]\n",
      " [ 259 1779]]\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8796    0.9648    0.9203      1962\n",
      "    positive     0.9627    0.8729    0.9156      2038\n",
      "\n",
      "    accuracy                         0.9180      4000\n",
      "   macro avg     0.9212    0.9189    0.9179      4000\n",
      "weighted avg     0.9219    0.9180    0.9179      4000\n",
      "\n",
      "=== Scalar metrics returned by Trainer ===\n",
      "eval_loss           : 0.3156\n",
      "eval_model_preparation_time: 0.0008\n",
      "eval_accuracy       : 0.9180\n",
      "eval_macro_f1       : 0.9179\n",
      "eval_macro_precision: 0.9212\n",
      "eval_macro_recall   : 0.9189\n",
      "eval_negative_precision: 0.8796\n",
      "eval_negative_recall: 0.9648\n",
      "eval_negative_f1    : 0.9203\n",
      "eval_negative_support: 1962.0000\n",
      "eval_positive_precision: 0.9627\n",
      "eval_positive_recall: 0.8729\n",
      "eval_positive_f1    : 0.9156\n",
      "eval_positive_support: 2038.0000\n",
      "eval_runtime        : 90.7454\n",
      "eval_samples_per_second: 44.0790\n",
      "eval_steps_per_second: 5.5100\n",
      "\n",
      "=== Cleaned DataFrame Row ===\n",
      "   eval_loss  eval_accuracy  eval_macro_f1  eval_macro_precision  \\\n",
      "0    0.31558          0.918       0.917933              0.921155   \n",
      "\n",
      "   eval_macro_recall  eval_negative_precision  eval_negative_recall  \\\n",
      "0           0.918873                 0.879647              0.964832   \n",
      "\n",
      "   eval_negative_f1  eval_negative_support  eval_positive_precision  \\\n",
      "0          0.920272                   1962                 0.962662   \n",
      "\n",
      "   eval_positive_recall  eval_positive_f1  eval_positive_support  data_amount  \\\n",
      "0              0.872915          0.915594                   2038         4281   \n",
      "\n",
      "       method  \n",
      "0  Randomized  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "from transformers import Trainer\n",
    "import pandas as pd\n",
    "CLASS_NAMES = [\"negative\", \"positive\"]  # adjust if you have more\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "\n",
    "    # overall (macro) metrics\n",
    "    macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    # per-class metrics\n",
    "    per_class = precision_recall_fscore_support(labels, preds, average=None, zero_division=0)\n",
    "    p_cls, r_cls, f1_cls, support_cls = per_class\n",
    "\n",
    "    # Flatten per-class metrics into scalars in the returned dict\n",
    "    metrics = {\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"macro_precision\": macro_p,\n",
    "        \"macro_recall\": macro_r,\n",
    "    }\n",
    "    for idx, name in enumerate(CLASS_NAMES):\n",
    "        metrics[f\"{name}_precision\"] = p_cls[idx]\n",
    "        metrics[f\"{name}_recall\"]    = r_cls[idx]\n",
    "        metrics[f\"{name}_f1\"]        = f1_cls[idx]\n",
    "        metrics[f\"{name}_support\"]   = support_cls[idx]\n",
    "\n",
    "    return metrics\n",
    "test_ds_reduced = test_ds_reduced.map(get_tokens, batch_size=16, batched=True)\n",
    "if \"labels\" not in test_ds_reduced.column_names:\n",
    "    test_ds_reduced = test_ds_reduced.rename_column(\"label\", \"labels\")\n",
    "\n",
    "evaluation_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_ds_reduced,\n",
    "    data_collator=default_data_collator,\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "metrics = evaluation_trainer.evaluate()\n",
    "pred_out = evaluation_trainer.predict(test_ds_reduced)\n",
    "\n",
    "logits = torch.tensor(pred_out.predictions, dtype=torch.float32)\n",
    "probs = torch.nn.functional.softmax(logits, dim=1) \n",
    "preds  = pred_out.predictions.argmax(axis=1)\n",
    "labels = pred_out.label_ids\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(labels, preds))\n",
    "\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(labels, preds, target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "print(\"=== Scalar metrics returned by Trainer ===\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k:20s}: {v:.4f}\")\n",
    "\n",
    "manual_metrics = compute_metrics((logits.numpy(), labels))\n",
    "\n",
    "# === Add clean row ===\n",
    "row = {\n",
    "    \"eval_loss\": pred_out.metrics.get(\"test_loss\", None),\n",
    "    \"eval_accuracy\": manual_metrics[\"accuracy\"],\n",
    "    \"eval_macro_f1\": manual_metrics[\"macro_f1\"],\n",
    "    \"eval_macro_precision\": manual_metrics[\"macro_precision\"],\n",
    "    \"eval_macro_recall\": manual_metrics[\"macro_recall\"],\n",
    "    \"eval_negative_precision\": manual_metrics[\"negative_precision\"],\n",
    "    \"eval_negative_recall\": manual_metrics[\"negative_recall\"],\n",
    "    \"eval_negative_f1\": manual_metrics[\"negative_f1\"],\n",
    "    \"eval_negative_support\": manual_metrics[\"negative_support\"],\n",
    "    \"eval_positive_precision\": manual_metrics[\"positive_precision\"],\n",
    "    \"eval_positive_recall\": manual_metrics[\"positive_recall\"],\n",
    "    \"eval_positive_f1\": manual_metrics[\"positive_f1\"],\n",
    "    \"eval_positive_support\": manual_metrics[\"positive_support\"],\n",
    "    \"data_amount\": len(train_ds_reduced),\n",
    "    \"method\": \"Randomized\",  # or \"Randomized\", etc.\n",
    "}\n",
    "\n",
    "df = pd.DataFrame([row])\n",
    "print(\"\\n=== Cleaned DataFrame Row ===\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "181b6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = \"results.xlsx\"\n",
    "sheet_name = \"results\"\n",
    "\n",
    "# Load existing or create new\n",
    "try:\n",
    "    existing_df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "    df = pd.concat([existing_df, df], ignore_index=True)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "# Save back\n",
    "df.to_excel(excel_path, index=False, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "054b6443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"save/model/randomized_p1_5000\")\n",
    "tokenizer.save_pretrained(\"save/tokenizer/randomized_p1_5000\")\n",
    "print(\"Model and tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6cece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Assume train_ds is your full unshuffled dataset\n",
    "train_size = len(train_ds)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "shuffled_indices = rng.permutation(train_size)\n",
    "\n",
    "# Get first 5000\n",
    "selected_indices = shuffled_indices[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e128dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# === Step 0: Recover selected indices ===\n",
    "# Assuming you originally had: train_ds.shuffle(seed=42).select(range(5000))\n",
    "# And mean_pooled_bert_embeddings are computed from the full train_ds before shuffle\n",
    "\n",
    "full_dataset_size = len(mean_pooled_bert_embeddings)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "shuffled_indices = rng.permutation(full_dataset_size)\n",
    "selected_indices = shuffled_indices[:1000]\n",
    "\n",
    "# === Step 1: Clustering ===\n",
    "scaler = RobustScaler()\n",
    "label_umap = UMAP(n_components=50, random_state=42)\n",
    "visual_umap = UMAP(n_components=2, random_state=42)\n",
    "clusterer = HDBSCAN(min_cluster_size=200)\n",
    "\n",
    "mean_pooled_bert_embeddings = np.load(\"bert_mean_pooled_embeddings.npy\")\n",
    "label_embeddings = label_umap.fit_transform(mean_pooled_bert_embeddings)\n",
    "label_embeddings = visual_umap.fit_transform(label_embeddings)\n",
    "label_embeddings = scaler.fit_transform(label_embeddings)\n",
    "\n",
    "labels = clusterer.fit_predict(label_embeddings)\n",
    "\n",
    "# === Step 2: Plot all points (faded) and selected points (highlighted) ===\n",
    "X_umap = label_embeddings  # Already 2D, no need to recompute\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "palette = sns.color_palette(\"colorblind\", n_colors=len(set(labels)))\n",
    "\n",
    "# Plot all samples with cluster labels, low alpha\n",
    "sns.scatterplot(\n",
    "    x=X_umap[:, 0], y=X_umap[:, 1],\n",
    "    hue=labels,\n",
    "    palette=palette,\n",
    "    legend='full',\n",
    "    alpha=0.3,\n",
    "    s=30\n",
    ")\n",
    "\n",
    "# Highlight selected samples\n",
    "selected_X = X_umap[selected_indices]\n",
    "selected_labels = labels[selected_indices]\n",
    "sns.scatterplot(\n",
    "    x=selected_X[:, 0], y=selected_X[:, 1],\n",
    "    hue=selected_labels,\n",
    "    palette=palette,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.6,\n",
    "    alpha=1.0,\n",
    "    s=70,\n",
    "    legend=False  # Suppress duplicate legend\n",
    ")\n",
    "\n",
    "plt.title(\"HDBSCAN Clusters with Selected Training Samples Highlighted\")\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
