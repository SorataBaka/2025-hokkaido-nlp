{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bdb0e32",
   "metadata": {},
   "source": [
    "# Load datasets, clean, and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7529873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"mteb/amazon_polarity\", cache_dir=\"caches/\")\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ce23cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(row):\n",
    "    text = row[\"text\"]\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    # Remove non-printable characters\n",
    "    text = re.sub(r\"[\\x00-\\x1F\\x7F]\", \" \", text)\n",
    "    # Replace multiple spaces/newlines with single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Optionally lowercase\n",
    "    text = text.strip()  # Don't lowercase if case matters\n",
    "    return {\n",
    "        \"text\": text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8558c8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DS length: 36000\n",
      "Test DS length: 4000\n"
     ]
    }
   ],
   "source": [
    "train_valid = train_ds.train_test_split(test_size=0.01, seed=42)\n",
    "test_valid = test_ds.train_test_split(test_size=0.01, seed=42)\n",
    "\n",
    "train_ds_reduced = train_valid[\"test\"]\n",
    "test_ds_reduced = test_valid[\"test\"]\n",
    "\n",
    "train_ds_reduced = train_ds_reduced.map(clean_text)\n",
    "test_ds_reduced = test_ds_reduced.map(clean_text)\n",
    "\n",
    "print(f\"Train DS length: {len(train_ds_reduced)}\")\n",
    "print(f\"Test DS length: {len(test_ds_reduced)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f168665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import math\n",
    "def select_stratified(dataset, num_samples):\n",
    "  concat_ds = []\n",
    "  positive_ds = dataset.filter(lambda x: x[\"label\"] == 1)\n",
    "  negative_ds = dataset.filter(lambda x: x[\"label\"] == 0)\n",
    "  \n",
    "  positive_count = len(positive_ds)\n",
    "  negative_count = len(negative_ds)\n",
    "  \n",
    "  print(f\"Positive samples: {positive_count}, Negative samples: {negative_count}\")\n",
    "  \n",
    "  total = len(dataset)\n",
    "  positive_ratio = positive_count / total if total > 0 else 0\n",
    "  negative_ratio = negative_count / total if total > 0 else 0\n",
    "  \n",
    "  positive_samples = math.ceil(num_samples * positive_ratio) if math.ceil(num_samples * positive_ratio) < positive_count else positive_count\n",
    "  negative_samples = math.ceil(num_samples * negative_ratio) if math.ceil(num_samples * negative_ratio) < negative_count else negative_count\n",
    "  \n",
    "  positive_subset = positive_ds.shuffle(seed=42).select(range(positive_samples))\n",
    "  negative_subset = negative_ds.shuffle(seed=42).select(range(negative_samples))\n",
    "  \n",
    "  concat_ds.append(positive_subset)\n",
    "  concat_ds.append(negative_subset)\n",
    "  print(f\"Ratio - Positive: {positive_ratio:.2f}, Negative: {negative_ratio:.2f}\")\n",
    "  print(f\"Selected {positive_samples} positive samples and {negative_samples} negative samples.\")\n",
    "  \n",
    "  \n",
    "  return concatenate_datasets(concat_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a10e968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text', 'label_text'],\n",
       "    num_rows: 2617\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_reduced = train_ds_reduced.shuffle(seed=42).select(range(2617))\n",
    "train_ds_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb9814d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"caches/\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"caches/\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "730033ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(batch):\n",
    "  tokens =  tokenizer(\n",
    "    batch[\"text\"],\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256\n",
    "  )\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "676be9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2617/2617 [00:00<00:00, 5656.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = train_ds_reduced.map(get_tokens, batch_size=16, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c997cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_split = tokenized_dataset.train_test_split(test_size=0.15, seed=42)\n",
    "\n",
    "final_dataset_train = final_dataset_split[\"train\"]\n",
    "final_dataset_test = final_dataset_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea338579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "dl = DataLoader(tokenized_dataset, batch_size=4, collate_fn=default_data_collator)\n",
    "\n",
    "for batch in dl:\n",
    "    print(batch['input_ids'].shape)  # ❌ Likely to crash or be malformed\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37cb0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gc import callbacks\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  num_train_epochs=3,\n",
    "  per_device_train_batch_size=8,\n",
    "  save_strategy=\"epoch\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=50,\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"eval_accuracy\",\n",
    "  greater_is_better=True,\n",
    "  eval_strategy=\"epoch\",\n",
    "  warmup_ratio=0.1,\n",
    "  weight_decay=0.01,\n",
    "  learning_rate=3e-5,\n",
    "  lr_scheduler_type=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2c511022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, default_data_collator\n",
    "from torch.optim import AdamW\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import EarlyStoppingCallback, Trainer\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\", pos_label=1, zero_division=0\n",
    "    )\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        \"eval_accuracy\": acc,\n",
    "        \"eval_precision\": precision,\n",
    "        \"eval_recall\": recall,\n",
    "        \"eval_f1\": f1,\n",
    "    }\n",
    "  \n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args = training_args,\n",
    "  train_dataset=final_dataset_train,\n",
    "  eval_dataset=final_dataset_test,\n",
    "  compute_metrics=compute_metrics,\n",
    "  data_collator=default_data_collator,\n",
    "  optimizers=(AdamW(model.parameters(), lr=5e-5), None),\n",
    "#   callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.01)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d2ace6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='834' max='834' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [834/834 11:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.308400</td>\n",
       "      <td>0.432546</td>\n",
       "      <td>0.877863</td>\n",
       "      <td>0.875576</td>\n",
       "      <td>0.900474</td>\n",
       "      <td>0.887850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.222400</td>\n",
       "      <td>0.465323</td>\n",
       "      <td>0.893130</td>\n",
       "      <td>0.951872</td>\n",
       "      <td>0.843602</td>\n",
       "      <td>0.894472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.580748</td>\n",
       "      <td>0.888041</td>\n",
       "      <td>0.907317</td>\n",
       "      <td>0.881517</td>\n",
       "      <td>0.894231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=834, training_loss=0.21936482991531883, metrics={'train_runtime': 688.64, 'train_samples_per_second': 9.689, 'train_steps_per_second': 1.211, 'total_flos': 877738480680960.0, 'train_loss': 0.21936482991531883, 'epoch': 3.0})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7698fb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentiment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Confusion Matrix ===\n",
      "[[1895   67]\n",
      " [ 248 1790]]\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8843    0.9659    0.9233      1962\n",
      "    positive     0.9639    0.8783    0.9191      2038\n",
      "\n",
      "    accuracy                         0.9213      4000\n",
      "   macro avg     0.9241    0.9221    0.9212      4000\n",
      "weighted avg     0.9249    0.9213    0.9212      4000\n",
      "\n",
      "=== Scalar metrics returned by Trainer ===\n",
      "eval_loss           : 0.3306\n",
      "eval_model_preparation_time: 0.0012\n",
      "eval_accuracy       : 0.9213\n",
      "eval_macro_f1       : 0.9212\n",
      "eval_macro_precision: 0.9241\n",
      "eval_macro_recall   : 0.9221\n",
      "eval_negative_precision: 0.8843\n",
      "eval_negative_recall: 0.9659\n",
      "eval_negative_f1    : 0.9233\n",
      "eval_negative_support: 1962.0000\n",
      "eval_positive_precision: 0.9639\n",
      "eval_positive_recall: 0.8783\n",
      "eval_positive_f1    : 0.9191\n",
      "eval_positive_support: 2038.0000\n",
      "eval_runtime        : 97.9071\n",
      "eval_samples_per_second: 40.8550\n",
      "eval_steps_per_second: 5.1070\n",
      "\n",
      "=== Cleaned DataFrame Row ===\n",
      "       method  stage  data_amount  eval_loss  eval_accuracy  \\\n",
      "0  Randomized      1         2617   0.330581        0.92125   \n",
      "\n",
      "   eval_macro_precision  eval_macro_recall  eval_macro_f1  \\\n",
      "0              0.924097           0.922082       0.921196   \n",
      "\n",
      "   eval_negative_precision  eval_negative_recall  eval_negative_f1  \\\n",
      "0                 0.884274              0.965851          0.923264   \n",
      "\n",
      "   eval_positive_precision  eval_positive_recall  eval_positive_f1  \n",
      "0                  0.96392              0.878312          0.919127  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "from transformers import Trainer\n",
    "import pandas as pd\n",
    "CLASS_NAMES = [\"negative\", \"positive\"]  # adjust if you have more\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "\n",
    "    # overall (macro) metrics\n",
    "    macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    # per-class metrics\n",
    "    per_class = precision_recall_fscore_support(labels, preds, average=None, zero_division=0)\n",
    "    p_cls, r_cls, f1_cls, support_cls = per_class\n",
    "\n",
    "    # Flatten per-class metrics into scalars in the returned dict\n",
    "    metrics = {\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"macro_precision\": macro_p,\n",
    "        \"macro_recall\": macro_r,\n",
    "    }\n",
    "    for idx, name in enumerate(CLASS_NAMES):\n",
    "        metrics[f\"{name}_precision\"] = p_cls[idx]\n",
    "        metrics[f\"{name}_recall\"]    = r_cls[idx]\n",
    "        metrics[f\"{name}_f1\"]        = f1_cls[idx]\n",
    "        metrics[f\"{name}_support\"]   = support_cls[idx]\n",
    "\n",
    "    return metrics\n",
    "test_ds_reduced = test_ds_reduced.map(get_tokens, batch_size=16, batched=True)\n",
    "if \"labels\" not in test_ds_reduced.column_names:\n",
    "    test_ds_reduced = test_ds_reduced.rename_column(\"label\", \"labels\")\n",
    "\n",
    "evaluation_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_ds_reduced,\n",
    "    data_collator=default_data_collator,\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "metrics = evaluation_trainer.evaluate()\n",
    "pred_out = evaluation_trainer.predict(test_ds_reduced)\n",
    "\n",
    "logits = torch.tensor(pred_out.predictions, dtype=torch.float32)\n",
    "probs = torch.nn.functional.softmax(logits, dim=1) \n",
    "preds  = pred_out.predictions.argmax(axis=1)\n",
    "labels = pred_out.label_ids\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(labels, preds))\n",
    "\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(labels, preds, target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "print(\"=== Scalar metrics returned by Trainer ===\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k:20s}: {v:.4f}\")\n",
    "\n",
    "manual_metrics = compute_metrics((logits.numpy(), labels))\n",
    "\n",
    "# === Add clean row ===\n",
    "row = {\n",
    "    \"method\": \"Randomized\",\n",
    "    \"stage\": 1,\n",
    "    \"data_amount\": len(train_ds_reduced),\n",
    "    \"eval_loss\": pred_out.metrics.get(\"test_loss\", None),\n",
    "    \"eval_accuracy\": manual_metrics[\"accuracy\"],\n",
    "    \"eval_macro_precision\": manual_metrics[\"macro_precision\"],\n",
    "    \"eval_macro_recall\": manual_metrics[\"macro_recall\"],\n",
    "    \"eval_macro_f1\": manual_metrics[\"macro_f1\"],\n",
    "    \"eval_negative_precision\": manual_metrics[\"negative_precision\"],\n",
    "    \"eval_negative_recall\": manual_metrics[\"negative_recall\"],\n",
    "    \"eval_negative_f1\": manual_metrics[\"negative_f1\"],\n",
    "    \"eval_positive_precision\": manual_metrics[\"positive_precision\"],\n",
    "    \"eval_positive_recall\": manual_metrics[\"positive_recall\"],\n",
    "    \"eval_positive_f1\": manual_metrics[\"positive_f1\"],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame([row])\n",
    "print(\"\\n=== Cleaned DataFrame Row ===\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "181b6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = \"results.xlsx\"\n",
    "sheet_name = \"results\"\n",
    "\n",
    "# Load existing or create new\n",
    "try:\n",
    "    existing_df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "    df = pd.concat([existing_df, df], ignore_index=True)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "# Save back\n",
    "df.to_excel(excel_path, index=False, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "054b6443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"save/model/randomized_p1_5000\")\n",
    "tokenizer.save_pretrained(\"save/tokenizer/randomized_p1_5000\")\n",
    "print(\"Model and tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7d6cece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Assume train_ds is your full unshuffled dataset\n",
    "train_size = len(train_ds)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "shuffled_indices = rng.permutation(train_size)\n",
    "\n",
    "# Get first 5000\n",
    "selected_indices = shuffled_indices[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0e128dfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_pooled_bert_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m silhouette_score, davies_bouldin_score, calinski_harabasz_score\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# === Step 0: Recover selected indices ===\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming you originally had: train_ds.shuffle(seed=42).select(range(5000))\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# And mean_pooled_bert_embeddings are computed from the full train_ds before shuffle\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m full_dataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmean_pooled_bert_embeddings\u001b[49m)\n\u001b[1;32m     14\u001b[0m rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     15\u001b[0m shuffled_indices \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mpermutation(full_dataset_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_pooled_bert_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# === Step 0: Recover selected indices ===\n",
    "# Assuming you originally had: train_ds.shuffle(seed=42).select(range(5000))\n",
    "# And mean_pooled_bert_embeddings are computed from the full train_ds before shuffle\n",
    "\n",
    "full_dataset_size = len(mean_pooled_bert_embeddings)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "shuffled_indices = rng.permutation(full_dataset_size)\n",
    "selected_indices = shuffled_indices[:1000]\n",
    "\n",
    "# === Step 1: Clustering ===\n",
    "scaler = RobustScaler()\n",
    "label_umap = UMAP(n_components=50, random_state=42)\n",
    "visual_umap = UMAP(n_components=2, random_state=42)\n",
    "clusterer = HDBSCAN(min_cluster_size=200)\n",
    "\n",
    "mean_pooled_bert_embeddings = np.load(\"bert_mean_pooled_embeddings.npy\")\n",
    "label_embeddings = label_umap.fit_transform(mean_pooled_bert_embeddings)\n",
    "label_embeddings = visual_umap.fit_transform(label_embeddings)\n",
    "label_embeddings = scaler.fit_transform(label_embeddings)\n",
    "\n",
    "labels = clusterer.fit_predict(label_embeddings)\n",
    "\n",
    "# === Step 2: Plot all points (faded) and selected points (highlighted) ===\n",
    "X_umap = label_embeddings  # Already 2D, no need to recompute\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "palette = sns.color_palette(\"colorblind\", n_colors=len(set(labels)))\n",
    "\n",
    "# Plot all samples with cluster labels, low alpha\n",
    "sns.scatterplot(\n",
    "    x=X_umap[:, 0], y=X_umap[:, 1],\n",
    "    hue=labels,\n",
    "    palette=palette,\n",
    "    legend='full',\n",
    "    alpha=0.3,\n",
    "    s=30\n",
    ")\n",
    "\n",
    "# Highlight selected samples\n",
    "selected_X = X_umap[selected_indices]\n",
    "selected_labels = labels[selected_indices]\n",
    "sns.scatterplot(\n",
    "    x=selected_X[:, 0], y=selected_X[:, 1],\n",
    "    hue=selected_labels,\n",
    "    palette=palette,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.6,\n",
    "    alpha=1.0,\n",
    "    s=70,\n",
    "    legend=False  # Suppress duplicate legend\n",
    ")\n",
    "\n",
    "plt.title(\"HDBSCAN Clusters with Selected Training Samples Highlighted\")\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
